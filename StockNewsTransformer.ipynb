{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-processing\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v7pJJcdj0vkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "raw_data = pd.read_excel('updated_pre_processed_news.xlsx')\n",
        "\n",
        "raw_data = raw_data[['text', 'sentiment_score']]\n",
        "print(raw_data)"
      ],
      "metadata": {
        "id": "LaiqeZNi1coW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e22304e-a5a6-4d2e-bcf8-60139bdcd136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  sentiment_score\n",
            "0      Chipotle, New Age Beverages, and Roku have all...              4.4\n",
            "1      Chipotle, New Age Beverages, and Roku have all...             -0.6\n",
            "2      Chipotle, New Age Beverages, and Roku have all...             -0.9\n",
            "3      Highly scalable platform, strategic inorganic ...             -0.3\n",
            "4      Globus Medical is upbeat about its growth pros...             -2.7\n",
            "...                                                  ...              ...\n",
            "14502  Brink's (BCO) came out with quarterly earnings...             -0.8\n",
            "14503  CAMBRIDGE, Mass., May 08, 2024 (GLOBE NEWSWIRE...             11.6\n",
            "14504  Amgen stock (NASDAQ NASDAQ : AMGN) has seen a ...              2.4\n",
            "14505  MONTREAL, May 08, 2024 (GLOBE NEWSWIRE) -- CN ...              1.0\n",
            "14506  NEW YORK, NY / ACCESSWIRE / May 8, 2024 / If y...             -0.4\n",
            "\n",
            "[14507 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "raw_data = raw_data.dropna()\n",
        "raw_data['sentiment_score'] = raw_data['sentiment_score'].astype(float)\n",
        "train_data, validation_data = train_test_split(raw_data, test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "Hap01qv0IU6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799513af-cfc0-4d59-a319-00d73fb8b822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-25cba5fed7fd>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  raw_data['sentiment_score'] = raw_data['sentiment_score'].astype(float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_JCXP2hbKhTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) For convenience, now change the created train/val/test data from Hugging Face’s dataset\n",
        "to pandas dataframe. Pre-process the data by removing the punctuation marks and stop words,\n",
        "and converting all words to lowercase. Note that the regular expression operations (doc) and\n",
        "nltk library might be useful.\n",
        "\n"
      ],
      "metadata": {
        "id": "aBAWYUT51e_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the list of stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a function to clean text data\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using a regular expression\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Join words back to string\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "raw_data['text'] = raw_data['text'].apply(clean_text)\n",
        "print(raw_data)\n",
        "train_data['text'] = train_data['text'].apply(clean_text)\n",
        "print(train_data)\n",
        "validation_data['text'] = validation_data['text'].apply(clean_text)\n",
        "print(validation_data)"
      ],
      "metadata": {
        "id": "z4VGIAXE1hqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8fc672-19d7-47dc-c3c4-185cb5d95723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  sentiment_score\n",
            "0      chipotle new age beverages roku soared 100 pas...              4.4\n",
            "1      chipotle new age beverages roku soared 100 pas...             -0.6\n",
            "2      chipotle new age beverages roku soared 100 pas...             -0.9\n",
            "3      highly scalable platform strategic inorganic c...             -0.3\n",
            "4      globus medical upbeat growth prospects current...             -2.7\n",
            "...                                                  ...              ...\n",
            "14502  brinks bco came quarterly earnings 152 per sha...             -0.8\n",
            "14503  cambridge mass may 08 2024 globe newswire eter...             11.6\n",
            "14504  amgen stock nasdaq nasdaq amgn seen solid 9 ri...              2.4\n",
            "14505  montreal may 08 2024 globe newswire cn tsx cnr...              1.0\n",
            "14506  new york ny accesswire may 8 2024 suffered los...             -0.4\n",
            "\n",
            "[13397 rows x 2 columns]\n",
            "                                                    text  sentiment_score\n",
            "8845   southfield michbusiness wireihs markit nyse in...             -1.0\n",
            "12256  lloyds banking group plc lselloy uk banking se...              0.5\n",
            "5719   moderna mrna shareholders hitting exits aggres...              8.6\n",
            "9275   shares nike nyse nke 8 wake fq3 earnings repor...             -3.1\n",
            "5955   soulstring media group covers wsgfs innovative...             -4.4\n",
            "...                                                  ...              ...\n",
            "13058  arnab ghosal named southern telecom chief oper...              1.1\n",
            "5727   dallas dec 23 2020 prnewswire moneygram intern...              0.0\n",
            "5945   san francisco dec 21 2020 prnewswire schubert ...              6.8\n",
            "961    lindsay corporations lnn focus growing infrast...             -1.8\n",
            "7967   smith wesson brands swbi seen solid earnings e...              0.4\n",
            "\n",
            "[12057 rows x 2 columns]\n",
            "                                                    text  sentiment_score\n",
            "4316   nashua nh june 29 2020 globe newswire icad inc...             -4.9\n",
            "13888  new report details latest app data trends mobi...              6.9\n",
            "5811   apple car talk heating rumors claim tech compa...              0.2\n",
            "462    article argues disney walmart even att would i...              0.9\n",
            "204                               sector etf report qaba              0.3\n",
            "...                                                  ...              ...\n",
            "951    german automaker daimler issued profit warning...             -0.7\n",
            "13717  intel corp intc shares rose 25 premarket tradi...             -1.8\n",
            "11817  industry leaders warned lithium element used m...             -3.1\n",
            "5959   name changes often associated desire change in...              0.1\n",
            "2198   glaxos gsk asthma medicine nucala gets fda app...              0.1\n",
            "\n",
            "[1340 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Convert the label into two types by the number of stars: P ositive ≥ 4, N egative ≤ 3.\n",
        "\n"
      ],
      "metadata": {
        "id": "sb9FiExn1c_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to categorize the ratings\n",
        "def categorize_sentiment(sentiment_score):\n",
        "    if sentiment_score >= 5:\n",
        "        return 1  # classifies significant price movements as 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "common_words = raw_data[raw_data['sentiment_score'] <= -20]\n",
        "\n",
        "# Apply the function to the sentiment_score column in both DataFrames\n",
        "train_data['sentiment_score'] = train_data['sentiment_score'].apply(categorize_sentiment)\n",
        "validation_data['sentiment_score'] = validation_data['sentiment_score'].apply(categorize_sentiment)\n",
        "\n",
        "# Print the DataFrames\n",
        "print(\"Train Data:\\n\", train_data)\n",
        "print(\"Validation Data:\\n\", validation_data)\n",
        "\n",
        "# Print the sum of the sentiment_score column in train_data\n",
        "print(\"Sum of sentiment scores in train data:\", train_data['sentiment_score'].sum())"
      ],
      "metadata": {
        "id": "e6GIa4Nr1HsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72bcf24-5cb8-4020-9e40-1937eb989ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data:\n",
            "                                                     text  sentiment_score\n",
            "8845   southfield michbusiness wireihs markit nyse in...                0\n",
            "12256  lloyds banking group plc lselloy uk banking se...                0\n",
            "5719   moderna mrna shareholders hitting exits aggres...                0\n",
            "9275   shares nike nyse nke 8 wake fq3 earnings repor...                0\n",
            "5955   soulstring media group covers wsgfs innovative...                0\n",
            "...                                                  ...              ...\n",
            "13058  arnab ghosal named southern telecom chief oper...                0\n",
            "5727   dallas dec 23 2020 prnewswire moneygram intern...                0\n",
            "5945   san francisco dec 21 2020 prnewswire schubert ...                0\n",
            "961    lindsay corporations lnn focus growing infrast...                0\n",
            "7967   smith wesson brands swbi seen solid earnings e...                0\n",
            "\n",
            "[12057 rows x 2 columns]\n",
            "Validation Data:\n",
            "                                                     text  sentiment_score\n",
            "4316   nashua nh june 29 2020 globe newswire icad inc...                0\n",
            "13888  new report details latest app data trends mobi...                0\n",
            "5811   apple car talk heating rumors claim tech compa...                0\n",
            "462    article argues disney walmart even att would i...                0\n",
            "204                               sector etf report qaba                0\n",
            "...                                                  ...              ...\n",
            "951    german automaker daimler issued profit warning...                0\n",
            "13717  intel corp intc shares rose 25 premarket tradi...                0\n",
            "11817  industry leaders warned lithium element used m...                0\n",
            "5959   name changes often associated desire change in...                0\n",
            "2198   glaxos gsk asthma medicine nucala gets fda app...                0\n",
            "\n",
            "[1340 rows x 2 columns]\n",
            "Sum of sentiment scores in train data: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Sample data for illustration\n",
        "\n",
        "# Combine all the text data into a single string\n",
        "all_text = ' '.join(common_words['text'].tolist())\n",
        "\n",
        "# Tokenize the text data into words (convert to lowercase and remove non-alphanumeric characters)\n",
        "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the most common words (e.g., top 10 most common words)\n",
        "most_common_words = word_counts.most_common(30)\n",
        "\n",
        "# Print the most common words\n",
        "print(\"Most common words:\")\n",
        "for word, count in most_common_words:\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nbDEBORKutL",
        "outputId": "81601300-7c6d-4fd9-df3a-3d7ff32cd4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words:\n",
            "company: 44\n",
            "inc: 40\n",
            "nasdaq: 30\n",
            "announced: 26\n",
            "stock: 26\n",
            "shares: 18\n",
            "today: 14\n",
            "2022: 14\n",
            "new: 13\n",
            "otc: 13\n",
            "per: 12\n",
            "share: 11\n",
            "companys: 11\n",
            "media: 10\n",
            "prnewswire: 10\n",
            "2023: 10\n",
            "march: 9\n",
            "loss: 9\n",
            "firm: 8\n",
            "agreement: 8\n",
            "globe: 8\n",
            "newswire: 8\n",
            "offering: 8\n",
            "million: 8\n",
            "investors: 8\n",
            "wednesday: 8\n",
            "2020: 8\n",
            "announce: 8\n",
            "higher: 8\n",
            "energy: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Input Data Preparation The input of the Transformer model should be a fixed-length\n",
        "review sequence where integer numbers represent words. Here you need to build vocabulary for\n",
        "the dataset and pad / truncate the review sequences to the same length.\n"
      ],
      "metadata": {
        "id": "27S-Mkxg1G7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Create tokenizer for the original text data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "# Filter out unknown words from the text data\n",
        "def filter_unknown_words(text, tokenizer):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word in tokenizer.word_index]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the filtering function to the text data\n",
        "train_data_filtered = train_data['text'].apply(filter_unknown_words, tokenizer=tokenizer)\n",
        "validation_data_filtered = validation_data['text'].apply(filter_unknown_words, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "# Create a new tokenizer for the filtered text data\n",
        "tokenizer_filtered = Tokenizer(num_words=10000)\n",
        "tokenizer_filtered.fit_on_texts(train_data_filtered)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "train_sequences_filtered = tokenizer_filtered.texts_to_sequences(train_data_filtered)\n",
        "validation_sequences_filtered = tokenizer_filtered.texts_to_sequences(validation_data_filtered)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_length = 32\n",
        "padded_train_filtered = pad_sequences(train_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "padded_validation_filtered = pad_sequences(validation_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "# Create the new tokenized DataFrames\n",
        "tokenized_train_df_filtered = pd.DataFrame({'sentiment_score': train_data['sentiment_score'], 'tokenized_text': list(padded_train_filtered)})\n",
        "tokenized_validation_df_filtered = pd.DataFrame({'sentiment_score': validation_data['sentiment_score'], 'tokenized_text': list(padded_validation_filtered)})\n",
        "\n",
        "print(tokenized_train_df_filtered)\n",
        "print(tokenized_validation_df_filtered)\n",
        "\n"
      ],
      "metadata": {
        "id": "60GXe9Ut1Gvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5768fbc6-af3d-40c9-f861-0c00450b13f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       sentiment_score                                     tokenized_text\n",
            "8845                 0  [8428, 4252, 4704, 17, 3373, 303, 255, 272, 82...\n",
            "12256                0  [5943, 708, 44, 422, 907, 708, 429, 2853, 168,...\n",
            "5719                 1  [2157, 2357, 146, 3007, 5944, 2358, 1751, 47, ...\n",
            "9275                 0  [11, 795, 17, 1597, 421, 1949, 6, 74, 62, 1227...\n",
            "5955                 0  [285, 44, 3374, 512, 2359, 2080, 24, 98, 260, ...\n",
            "...                ...                                                ...\n",
            "13058                0  [513, 1023, 3140, 158, 274, 218, 1401, 68, 260...\n",
            "5727                 0  [1157, 68, 127, 18, 23, 4014, 122, 3, 9, 4821,...\n",
            "5945                 1  [318, 919, 68, 83, 18, 23, 7283, 7284, 7285, 6...\n",
            "961                  0  [2228, 278, 357, 463, 25, 350, 647, 2083, 167,...\n",
            "7967                 0  [1152, 5020, 309, 5614, 121, 55, 6, 52, 170, 1...\n",
            "\n",
            "[12057 rows x 2 columns]\n",
            "       sentiment_score                                     tokenized_text\n",
            "4316                 0  [9654, 42, 598, 18, 49, 51, 3, 9, 35, 384, 48,...\n",
            "13888                1  [5, 74, 1020, 135, 719, 111, 549, 638, 608, 86...\n",
            "5811                 0  [264, 1110, 4239, 5327, 2244, 433, 1, 373, 895...\n",
            "462                  0  [1536, 6169, 663, 950, 541, 689, 280, 3763, 16...\n",
            "204                  0  [429, 194, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
            "...                ...                                                ...\n",
            "951                  0  [1316, 2585, 7679, 375, 380, 3248, 601, 2922, ...\n",
            "13717                0  [1131, 24, 2725, 11, 956, 183, 1739, 37, 149, ...\n",
            "11817                0  [58, 1126, 4135, 831, 7517, 602, 1653, 714, 95...\n",
            "5959                 0  [691, 1161, 1999, 1396, 739, 141, 525, 1, 409,...\n",
            "2198                 0  [3010, 9225, 1653, 1440, 456, 432, 411, 4831, ...\n",
            "\n",
            "[1340 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Transformer Implementation Implement a Transformer model which is composed of\n",
        "an encoder network (i.e., multi-head self-attention layers) and a prediction head mapping the\n",
        "hidden representation of input sequence into the label space (i.e., three classes). Find more\n",
        "details about Transformer in paper. You may need to implement positional embeddings, a\n",
        "vocabulary embedding table, and mask indicators for padded tokens. PyTorch is recommended\n",
        "for model implementation."
      ],
      "metadata": {
        "id": "IZ3n3Zif1Glg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)].detach()\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_tokens, d_model, nhead, num_classes, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.d_model = d_model  # Set as an instance attribute\n",
        "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.fc(output.mean(dim=1))\n",
        "        return output\n",
        "\n",
        "def create_padding_mask(seq_length, batch_size, num_heads):\n",
        "    # seq_length: length of the sequence\n",
        "    # num_heads: number of attention heads\n",
        "    # batch_size: size of the batch\n",
        "    # Return a mask tensor of shape [num_heads * seq_length, batch_size, batch_size]\n",
        "    mask = torch.zeros(num_heads * seq_length, batch_size, batch_size, dtype=torch.bool)\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bMh6RcEO1GbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Model Training and Finetuning\n",
        "\n",
        "(1) Train the model with SGD (or Adam/AdamW) optimizer using mini-batch fashion\n",
        "based on the training dataset."
      ],
      "metadata": {
        "id": "GAAjCbWU1GQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def compute_confusion_matrix_elements(predicted, labels, positive_label=1):\n",
        "    \"\"\"\n",
        "    Computes the number of true positives and false negatives.\n",
        "\n",
        "    Args:\n",
        "    - predicted (torch.Tensor): Predicted labels.\n",
        "    - labels (torch.Tensor): Actual labels.\n",
        "    - positive_label (int): The label considered as positive.\n",
        "\n",
        "    Returns:\n",
        "    - true_positives (int): Number of true positives.\n",
        "    - actual_positives (int): Number of actual positives.\n",
        "    \"\"\"\n",
        "    true_positives = ((predicted == positive_label) & (labels == positive_label)).sum().item()\n",
        "    actual_positives = (labels == positive_label).sum().item()\n",
        "    return true_positives, actual_positives\n",
        "\n",
        "\n",
        "# Parameters for the model\n",
        "num_tokens = 10000  # Number of distinct tokens in the vocabulary\n",
        "d_model = 512       # Dimension of the model (typically 512, 768, or 1024 in Transformers)\n",
        "nhead = 8        # Number of attention heads\n",
        "num_classes = 2     # Number of output classes\n",
        "num_layers = 1\n",
        "seq_length = 32\n",
        "batch_size = 32\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_length = seq_length\n",
        "padded_train_filtered = pad_sequences(train_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "padded_validation_filtered = pad_sequences(validation_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Create the new tokenized DataFrames\n",
        "tokenized_train_df_filtered = pd.DataFrame({'sentiment_score': train_data['sentiment_score'], 'tokenized_text': list(padded_train_filtered)})\n",
        "tokenized_validation_df_filtered = pd.DataFrame({'sentiment_score': validation_data['sentiment_score'], 'tokenized_text': list(padded_validation_filtered)})\n",
        "\n",
        "train_labels = torch.tensor(tokenized_train_df_filtered['sentiment_score'].values)\n",
        "train_inputs = torch.tensor(np.array(tokenized_train_df_filtered['tokenized_text'].tolist()))\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "validation_labels = torch.tensor(tokenized_validation_df_filtered['sentiment_score'].values)\n",
        "validation_inputs = torch.tensor(np.array(tokenized_validation_df_filtered['tokenized_text'].tolist()))\n",
        "validation_dataset = TensorDataset(validation_inputs, validation_labels)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Assume TransformerModel is defined correctly elsewhere\n",
        "model = TransformerModel(num_tokens=num_tokens, d_model=d_model, nhead=nhead, num_classes=num_classes, num_layers=num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "training_loss = []\n",
        "validation_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Training mode\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        src_mask = create_padding_mask(seq_length, batch_size, nhead)\n",
        "\n",
        "        outputs = model(inputs, src_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    training_loss.append(total_loss / len(train_loader))\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "\n",
        "    # Validation phase\n",
        "    # Other parts of the code remain the same\n",
        "\n",
        "# Validation phase\n",
        "model.eval()  # Evaluation mode\n",
        "total_accuracy = 0\n",
        "total_samples = 0\n",
        "\n",
        "total_true_positives = 0\n",
        "total_actual_positives = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for inputs, labels in validation_loader:\n",
        "        src_mask = create_padding_mask(seq_length, batch_size, nhead)\n",
        "\n",
        "        outputs = model(inputs, src_mask)\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted classes\n",
        "        total_samples += labels.size(0)\n",
        "        total_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "        # Compute true positives and actual positives\n",
        "        true_positives, actual_positives = compute_confusion_matrix_elements(predicted, labels)\n",
        "        total_true_positives += true_positives\n",
        "        total_actual_positives += actual_positives\n",
        "\n",
        "epoch_accuracy = total_accuracy / total_samples\n",
        "validation_accuracy.append(epoch_accuracy)\n",
        "print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "# Print true positives over actual positives\n",
        "if total_actual_positives > 0:\n",
        "    recall = total_true_positives / total_actual_positives\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, True Positives: {total_true_positives}, Actual Positives: {total_actual_positives}, Recall: {recall:.4f}\")\n",
        "else:\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, True Positives: {total_true_positives}, Actual Positives: {total_actual_positives}, Recall: Undefined (no actual positives)\")\n"
      ],
      "metadata": {
        "id": "5gQBShzW1GCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58d86f1-6edf-4500-c1d4-a7178ca8df2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 0.2550964205664523\n",
            "Epoch 2/10, Training Loss: 0.24287873304429206\n",
            "Epoch 3/10, Training Loss: 0.23484046437202932\n",
            "Epoch 4/10, Training Loss: 0.22799629293025492\n",
            "Epoch 5/10, Training Loss: 0.2200131307088552\n",
            "Epoch 6/10, Training Loss: 0.2121168313835292\n",
            "Epoch 7/10, Training Loss: 0.2048777397523852\n",
            "Epoch 8/10, Training Loss: 0.19493003110302257\n",
            "Epoch 9/10, Training Loss: 0.18640811459973772\n",
            "Epoch 10/10, Training Loss: 0.17900574086432128\n",
            "Epoch 10/10, Validation Accuracy: 0.9238\n",
            "Epoch 10/10, True Positives: 7, Actual Positives: 81, Recall: 0.0864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Plot two curves across training process, where the x-axis is the training epochs, and the\n",
        "y-axis is the training loss or validation accuracy."
      ],
      "metadata": {
        "id": "0ai4gn0B1F12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the 'epochs' list based on the actual number of epochs recorded in 'training_loss'\n",
        "epochs = range(1, len(training_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))  # Set the figure size\n",
        "\n",
        "# Plot training loss\n",
        "plt.plot(epochs, training_loss, 'r-o', label='Training Loss', linewidth=2, markersize=8)\n",
        "\n",
        "# Ensure that 'validation_accuracy' has been recorded for the same number of epochs\n",
        "if len(validation_accuracy) == len(training_loss):\n",
        "    # Plot validation accuracy\n",
        "    plt.plot(epochs, validation_accuracy, 'g-^', label='Validation Accuracy', linewidth=2, markersize=8)\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Training Loss and Validation Accuracy per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Metrics')\n",
        "plt.grid(True)\n",
        "\n",
        "# Add a legend to specify which line is which\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qqeptd0h1FoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4064b9d0-d079-49bf-ce0f-abbb330dfc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Finetune the transformer model, and save the best model with the highest validation\n",
        "accuracy."
      ],
      "metadata": {
        "id": "a0KdCnsA1FRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters to test\n",
        "n_heads_options = [8, 16]\n",
        "learning_rates = [0.001, 0.01]\n",
        "hidden_dims = [256, 512]\n",
        "num_epochs_options = [10, 30]\n",
        "\n",
        "# Placeholder for the best configuration\n",
        "best_accuracy = 0\n",
        "best_config = None\n",
        "\n",
        "for nhead in n_heads_options:\n",
        "    for lr in learning_rates:\n",
        "        for d_model in hidden_dims:\n",
        "            for num_epochs in num_epochs_options:\n",
        "                print(f\"Training with {nhead} heads, LR={lr}, Hidden Dim={d_model}, Epochs={num_epochs}\")\n",
        "\n",
        "                # Initialize model\n",
        "                model = TransformerModel(num_tokens=10000, d_model=d_model, nhead=nhead, num_classes=2, num_layers=1)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                # Train the model\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    for inputs, labels in train_loader:\n",
        "                        src_mask = create_padding_mask(seq_length, batch_size, nhead)\n",
        "                        outputs = model(inputs, src_mask)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Validate the model\n",
        "                model.eval()\n",
        "                total_accuracy = 0\n",
        "                total_samples = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in validation_loader:\n",
        "                        src_mask = create_padding_mask(seq_length, batch_size, nhead)\n",
        "                        outputs = model(inputs, src_mask)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total_samples += labels.size(0)\n",
        "                        total_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "                accuracy = total_accuracy / total_samples\n",
        "                print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                # Update best model\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_config = (nhead, lr, d_model, num_epochs)\n",
        "\n",
        "# Output the best configuration\n",
        "print(f\"Best configuration: {best_config} with accuracy {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "IGXHc9Tv1Ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "4dc55835-836b-4184-9164-959bbd820365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 8 heads, LR=0.001, Hidden Dim=256, Epochs=10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9271\n",
            "Training with 8 heads, LR=0.001, Hidden Dim=256, Epochs=30\n",
            "Validation Accuracy: 0.9167\n",
            "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10\n",
            "Validation Accuracy: 0.9062\n",
            "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=30\n",
            "Validation Accuracy: 0.9010\n",
            "Training with 8 heads, LR=0.01, Hidden Dim=256, Epochs=10\n",
            "Validation Accuracy: 0.8958\n",
            "Training with 8 heads, LR=0.01, Hidden Dim=256, Epochs=30\n",
            "Validation Accuracy: 0.9062\n",
            "Training with 8 heads, LR=0.01, Hidden Dim=512, Epochs=10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-56ccac0313fe>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-9695d31396e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    720\u001b[0m     def _sa_block(self, x: Tensor,\n\u001b[1;32m    721\u001b[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 722\u001b[0;31m         x = self.self_attn(x, x, x,\n\u001b[0m\u001b[1;32m    723\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5335\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5336\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4857\u001b[0m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4858\u001b[0m             \u001b[0;31m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4859\u001b[0;31m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4860\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4861\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I ran a tuning session to coarse tune the hyperparameters: number of epochs (10 vs 30), number of attention heads (8 vs 16), learning rate (0.001 vs 0.01), and hidden dimensions (256 vs 512).  \n",
        "\n",
        "Here are the results:\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=256, Epochs=10\n",
        "Validation Accuracy: 0.8010\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=256, Epochs=30\n",
        "Validation Accuracy: 0.7573\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10\n",
        "Validation Accuracy: 0.8031:\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=30\n",
        "Validation Accuracy: 0.7719\n",
        "\n",
        "Training with 8 heads, LR=0.01, Hidden Dim=256, Epochs=10\n",
        "Validation Accuracy: 0.6188\n",
        "\n",
        "Training with 8 heads, LR=0.01, Hidden Dim=256, Epochs=30\n",
        "Validation Accuracy: 0.6490\n",
        "\n",
        "Training with 8 heads, LR=0.01, Hidden Dim=512, Epochs=10\n",
        "Validation Accuracy: 0.6031\n",
        "\n",
        "Training with 8 heads, LR=0.01, Hidden Dim=512, Epochs=30\n",
        "Validation Accuracy: 0.6188\n",
        "\n",
        "Training with 16 heads, LR=0.001, Hidden Dim=256, Epochs=10\n",
        "Validation Accuracy: 0.7896\n",
        "\n",
        "Training with 16 heads, LR=0.001, Hidden Dim=256, Epochs=30\n",
        "Validation Accuracy: 0.7708\n",
        "\n",
        "Training with 16 heads, LR=0.001, Hidden Dim=512, Epochs=10\n",
        "Validation Accuracy: 0.7896\n",
        "\n",
        "Training with 16 heads, LR=0.001, Hidden Dim=512, Epochs=30\n",
        "Validation Accuracy: 0.7760\n",
        "\n",
        "Training with 16 heads, LR=0.01, Hidden Dim=256, Epochs=10\n",
        "Validation Accuracy: 0.6198\n",
        "\n",
        "Training with 16 heads, LR=0.01, Hidden Dim=256, Epochs=30\n",
        "Validation Accuracy: 0.6573\n",
        "\n",
        "Training with 16 heads, LR=0.01, Hidden Dim=512, Epochs=10\n",
        "Validation Accuracy: 0.6073\n",
        "\n",
        "Training with 16 heads, LR=0.01, Hidden Dim=512, Epochs=30\n",
        "Validation Accuracy: 0.6406\n",
        "\n",
        "Best configuration: (8, 0.001, 512, 10) with accuracy 0.8031\n",
        "\n",
        "I found that the learning rate of 0.001 produced significantly better validation accuracy than did the rate of 0.01.  When the learning rate was 0.001, 30 epochs performed better than 10 epochs, although when the learning rate was 0.01, 10 epochs performed better, likely due to the fact that with such a high learning rate, 30 epochs overtrained the model.  Hidden dimension of 512 performance was quite comparable with that of 256.  There was also no clear winner between 16 attention heads or 8 heads.  This first round of hyper-parameter tuning reported that the optimal configuration was:\n",
        "\n",
        "10 epochs (shorter), 8 heads, learning rate of 0.001 (lower), and 512 hidden dimensions (higher).  These configurations produced a validation accuracy of 80.31%. (992 samples total since the last incomplete batch was dropped)\n",
        "\n",
        "I will continue tuning using the following configuration:\n",
        "\n",
        "30 epochs, 8 attention heads, 0.001 learning rate, 512 hidden dimensions."
      ],
      "metadata": {
        "id": "awNdYcZkUU3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try tuning parameters such as batch size, sequence length."
      ],
      "metadata": {
        "id": "bhatl24ZOKuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I decided to try different values of batch size and sequence length.  I hypothesized that a shorter sequence might actually produce better accuracy since important words are likely written earlier in the sequence.  The results are as follows:\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=8, Sequence Length=8\n",
        "Validation Accuracy: 0.6730\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=8, Sequence Length=16\n",
        "Validation Accuracy: 0.6640\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=8, Sequence Length=32\n",
        "Validation Accuracy: 0.6500\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=8, Sequence Length=64\n",
        "Validation Accuracy: 0.6690\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=8, Sequence Length=128\n",
        "Validation Accuracy: 0.6710\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=32, Sequence Length=8\n",
        "Validation Accuracy: 0.7480\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=32, Sequence Length=16\n",
        "Validation Accuracy: 0.7762\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=32, Sequence Length=32\n",
        "Validation Accuracy: 0.8014\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=32, Sequence Length=64\n",
        "Validation Accuracy: 0.8115\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=32, Sequence Length=128\n",
        "Validation Accuracy: 0.8175\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=128, Sequence Length=8\n",
        "Validation Accuracy: 0.7946\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=128, Sequence Length=16\n",
        "Validation Accuracy: 0.8092\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=128, Sequence Length=32\n",
        "Validation Accuracy: 0.8013\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=128, Sequence Length=64\n",
        "Validation Accuracy: 0.8036\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=128, Sequence Length=128\n",
        "Validation Accuracy: 0.7868\n",
        "\n",
        "Best configuration: (8, 0.001, 512, 10, 32, 128) with accuracy 0.8175\n",
        "\n",
        "Smaller batch size tended to perform worse than larger batch size, and longer sequence length actually tended to perform much better than a shorter sequence length, contrary to my hypothesis.  The optimal Configuration is now as follows:\n",
        "Attention Heads:8\n",
        "Learning Rate: 0.001\n",
        "Hidden Dimensions: 512\n",
        "Epochs: 10\n",
        "Batch size: 32\n",
        "Sequence Length: 128\n",
        "Which produced a total validation accuracy of 81.75%"
      ],
      "metadata": {
        "id": "XY7xbPCio8w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, I want to see if adding attention layers will improve or degrade the performance of the transformer model.  I will try 1, 2, 3, and 4 layers:"
      ],
      "metadata": {
        "id": "-EsoB_W3zWdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:  I noticed that I had made a mistake earlier in the code and was not actually changing the sequence length when I thought I was, so I have to tune that parameter again:"
      ],
      "metadata": {
        "id": "XfjJIl_Irufq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the hyperparameters to test\n",
        "n_heads_options = [8]\n",
        "learning_rates = [0.001]\n",
        "hidden_dims = [512]\n",
        "num_epochs_options = [7, 8, 9, 10]\n",
        "batch_sizes = [32]\n",
        "seq_lengths = [32]\n",
        "\n",
        "# Placeholder for the best configuration\n",
        "best_accuracy = 0\n",
        "best_config = None\n",
        "\n",
        "for nhead in n_heads_options:\n",
        "    for lr in learning_rates:\n",
        "        for d_model in hidden_dims:\n",
        "            for num_epochs in num_epochs_options:\n",
        "                for batch_size in batch_sizes:\n",
        "                    for seq_length in seq_lengths:\n",
        "                        print(f\"Training with {nhead} heads, LR={lr}, Hidden Dim={d_model}, Epochs={num_epochs}, Batch Size={batch_size}, Sequence Length={seq_length}\")\n",
        "\n",
        "                        # Pad sequences to the current sequence length\n",
        "                        max_length = seq_length\n",
        "                        padded_train_filtered = pad_sequences(train_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "                        padded_validation_filtered = pad_sequences(validation_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "                        # Create the new tokenized DataFrames\n",
        "                        tokenized_train_df_filtered = pd.DataFrame({'sentiment_score': train_data['sentiment_score'], 'tokenized_text': list(padded_train_filtered)})\n",
        "                        tokenized_validation_df_filtered = pd.DataFrame({'sentiment_score': validation_data['sentiment_score'], 'tokenized_text': list(padded_validation_filtered)})\n",
        "\n",
        "                        # Convert DataFrame columns to tensors more efficiently\n",
        "                        train_labels = torch.tensor(tokenized_train_df_filtered['sentiment_score'].values)\n",
        "                        train_inputs = torch.tensor(np.array(tokenized_train_df_filtered['tokenized_text'].tolist()))\n",
        "                        train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "                        validation_labels = torch.tensor(tokenized_validation_df_filtered['sentiment_score'].values)\n",
        "                        validation_inputs = torch.tensor(np.array(tokenized_validation_df_filtered['tokenized_text'].tolist()))\n",
        "                        validation_dataset = TensorDataset(validation_inputs, validation_labels)\n",
        "                        validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "                        # Initialize model\n",
        "                        model = TransformerModel(num_tokens=10000, d_model=d_model, nhead=nhead, num_classes=2, num_layers=1)\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                        # Train the model\n",
        "                        for epoch in range(num_epochs):\n",
        "                            model.train()\n",
        "                            for inputs, labels in train_loader:\n",
        "                                src_mask = create_padding_mask(inputs.size(1), inputs.size(0), nhead)\n",
        "                                outputs = model(inputs, src_mask)\n",
        "                                loss = criterion(outputs, labels)\n",
        "\n",
        "                                optimizer.zero_grad()\n",
        "                                loss.backward()\n",
        "                                optimizer.step()\n",
        "\n",
        "                        # Validate the model\n",
        "                        model.eval()\n",
        "                        total_accuracy = 0\n",
        "                        total_samples = 0\n",
        "                        with torch.no_grad():\n",
        "                            for inputs, labels in validation_loader:\n",
        "                                src_mask = create_padding_mask(inputs.size(1), inputs.size(0), nhead)\n",
        "                                outputs = model(inputs, src_mask)\n",
        "                                _, predicted = torch.max(outputs.data, 1)\n",
        "                                total_samples += labels.size(0)\n",
        "                                total_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "                        accuracy = total_accuracy / total_samples if total_samples != 0 else 0\n",
        "                        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                        # Update best model\n",
        "                        if accuracy > best_accuracy:\n",
        "                            best_accuracy = accuracy\n",
        "                            best_config = (nhead, lr, d_model, num_epochs, batch_size, seq_length)\n",
        "\n",
        "# Output the best configuration\n",
        "print(f\"Best configuration: {best_config} with accuracy {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "CtCJa_D0wKY0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "502e30a9-9651-4b48-950a-5943feb42765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=32\n",
            "Validation Accuracy: 0.9062\n",
            "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=8, Batch Size=32, Sequence Length=32\n",
            "Validation Accuracy: 0.9152\n",
            "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=9, Batch Size=32, Sequence Length=32\n",
            "Validation Accuracy: 0.9062\n",
            "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=10, Batch Size=32, Sequence Length=32\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-7863f9afb4c6>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=8\n",
        "Validation Accuracy: 0.6865\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=16\n",
        "Validation Accuracy: 0.7298\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=32\n",
        "Validation Accuracy: 0.7611\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=64\n",
        "Validation Accuracy: 0.8226\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=96\n",
        "Validation Accuracy: 0.7833\n",
        "\n",
        "Training with 8 heads, LR=0.001, Hidden Dim=512, Epochs=7, Batch Size=32, Sequence Length=128\n",
        "Validation Accuracy: 0.7752\n",
        "\n",
        "The clear winner is a sequence length of 64 words/tokens with a validation accuracy of 82.26%.\n",
        "\n",
        "Finally, we have the optimal hyperparameters:\n",
        "\n",
        "n_heads = 8\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "hidden_dim = 512\n",
        "\n",
        "num_epochs = 7\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "seq_length = 64\n",
        "\n",
        "num_layers = 1\n",
        "\n",
        "Hopefully, we can get around 80% accuracy on the test set.\n"
      ],
      "metadata": {
        "id": "A93v-zRL72VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Test Result Analysis\n",
        "(1) Load the best model saved above and report the accuracy of the model on the test\n",
        "dataset."
      ],
      "metadata": {
        "id": "LVEI65ENbHWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save the optimal model:"
      ],
      "metadata": {
        "id": "XCNRWVYOcSTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal hyperparameters:\n",
        "n_heads = 8\n",
        "learning_rate = 0.001\n",
        "hidden_dim = 512\n",
        "num_epochs = 7\n",
        "batch_size = 32\n",
        "seq_length = 64\n",
        "num_layers = 1\n",
        "\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_length = seq_length\n",
        "padded_train_filtered = pad_sequences(train_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "padded_validation_filtered = pad_sequences(validation_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "padded_test_filtered = pad_sequences(test_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Create the new tokenized DataFrames\n",
        "tokenized_train_df_filtered = pd.DataFrame({'label': train_data['label'], 'tokenized_text': list(padded_train_filtered)})\n",
        "tokenized_validation_df_filtered = pd.DataFrame({'label': validation_data['label'], 'tokenized_text': list(padded_validation_filtered)})\n",
        "tokenized_test_df_filtered = pd.DataFrame({'label': test_data['label'], 'tokenized_text': list(padded_test_filtered)})\n",
        "\n",
        " # Convert DataFrame columns to tensors more efficiently\n",
        "train_labels = torch.tensor(tokenized_train_df_filtered['label'].values)\n",
        "train_inputs = torch.tensor(np.array(tokenized_train_df_filtered['tokenized_text'].tolist()))\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "validation_labels = torch.tensor(tokenized_validation_df_filtered['label'].values)\n",
        "validation_inputs = torch.tensor(np.array(tokenized_validation_df_filtered['tokenized_text'].tolist()))\n",
        "validation_dataset = TensorDataset(validation_inputs, validation_labels)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "# Initialize model\n",
        "model = TransformerModel(num_tokens=10000, d_model=hidden_dim, nhead=n_heads, num_classes=2, num_layers=num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "    for inputs, labels in train_loader:\n",
        "        src_mask = create_padding_mask(inputs.size(1), inputs.size(0), n_heads)\n",
        "        outputs = model(inputs, src_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()  # Accumulate the loss for the batch\n",
        "\n",
        "    # Print the training loss for the epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
        "print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "IsTOe5ZDls-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbbbd3d-b66e-4c38-9209-5766db0d2687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7, Training Loss: 2.0278162360191345\n",
            "Epoch 2/7, Training Loss: 0.925600012143453\n",
            "Epoch 3/7, Training Loss: 0.8801148335138956\n",
            "Epoch 4/7, Training Loss: 0.7616366545359293\n",
            "Epoch 5/7, Training Loss: 0.7288298805554708\n",
            "Epoch 6/7, Training Loss: 0.6478529771169027\n",
            "Epoch 7/7, Training Loss: 0.6674107710520426\n",
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use the saved model to make predictions on the test data:"
      ],
      "metadata": {
        "id": "TXEXBJVvcqHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 64\n",
        "batch_size = 32\n",
        "\n",
        "max_length = seq_length\n",
        "padded_test_filtered = pad_sequences(test_sequences_filtered, maxlen=max_length, padding='post', truncating='post')\n",
        "tokenized_test_df_filtered = pd.DataFrame({'label': test_data['label'], 'tokenized_text': list(padded_test_filtered)})\n",
        "\n",
        " #Load Test Data\n",
        "# Convert DataFrame columns to tensors more efficiently\n",
        "test_labels = torch.tensor(tokenized_test_df_filtered['label'].values)\n",
        "test_inputs = torch.tensor(np.array(tokenized_test_df_filtered['tokenized_text'].tolist()))\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        " # Validate the model\n",
        "model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Validate the model\n",
        "total_accuracy = 0\n",
        "total_samples = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        src_mask = create_padding_mask(inputs.size(1), inputs.size(0), n_heads)\n",
        "        outputs = model(inputs, src_mask)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        total_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = total_accuracy / total_samples\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-AhqTnOdD8g",
        "outputId": "b36cfbee-95c4-4e9f-97ca-e1ee90691379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.4173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Test Accuracy: 81.00%.  I think that's pretty good considering the subjective nature of the review content and ratings."
      ],
      "metadata": {
        "id": "VCfKJqMIfEJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) What are the impacts of hyper-parameters, such as the hidden dimension and the\n",
        "number of attention layers, on the Transformer?\n",
        "\n",
        "From the experiment, I can conclude that these are likely the impacts of hyperparameters on the transformer:\n",
        "\n",
        "Learning rate:  A high learning rate can help the model to learn optimal weights more quickly, but setting it too high can cause the model to miss the point entirely, leading to wildly high error rates and training loss.  The number of layers affects the learning rate:  more layers require a lower learning rate.  A learning rate that works well for a single-layer network is likely too high for a multi-layer transformer.  It is important to find a learning rate that is not too high or too low, and is also custom fit to the number of attention layers in the transformer.\n",
        "\n",
        "Hidden dimension:  The hidden dimension really did not seem to affect the network all that much.  I tried 512 and 256 hidden dimensions and saw no significant difference.  Some more investigation into this parameter my be pertinent.\n",
        "\n",
        "Number of attention layers:  The number of attention layers greatly affects the speed with which the transformer can be trained.  A transformer with more layers takes much longer to train than a transformer with fewer layers.  For this simple binary natural-language classifier, I think one layer was sufficient, but more layers might be necessary for more complex tasks such as generative natural language tasks.\n",
        "\n",
        "Training Epochs:  It is important to find the proper number of epochs to train the transformer.  A single-layer network can be sufficiently trained in fewer than 10 epochs, and any more than that is wasted time, and can even degrade performance on unseen data.  A multi-layer network takes more epochs at a lower training rate to see peak accuracy.\n",
        "\n",
        "Attention heads:  When comparing models with 8 vs 16 attention heads, I saw no significant difference between the performance of the two models, so I opted to keep the configuration of the 8 default heads, as recommended by the research paper in the link. There was no need to use more heads than necessary, as it would just slow down training performance.\n",
        "\n",
        "Sequence Length:  While I originally hypothesized that the sequence length not need to be very long to accurately predict whether the review was positive or negative, it turned out that a sequence length of 64 words had the best performance  when compared to shorter sequences.  Although, needlessly long sequences of 96 or 128 words performed worse than 64 words.\n",
        "\n",
        "Batch Size:  I originally thought that a smaller batch size would be better, as the gradient would be calculated more frequently, but it turns out that batching in batches of 32 samples is actually optimal.  I think this is a good balance between smoothness of gradient descent and updating frequently enough."
      ],
      "metadata": {
        "id": "v9-sgfdMvnKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "\n",
        "API_KEY = '14ee93fb9b974790a5d784ec5b8622ad'  # TwelveData API Key\n",
        "endpoint = 'https://api.twelvedata.com/time_series'\n",
        "\n",
        "# Load the dataset\n",
        "news = pd.read_excel(\"pre_processed_news.xlsx\")\n",
        "\n",
        "# Add new columns for the required price information\n",
        "news['open'] = None\n",
        "news['five_minutes_prior_price'] = None\n",
        "news['concurrent_price'] = None\n",
        "news['close'] = None\n",
        "news['low'] = None\n",
        "news['high'] = None\n",
        "news['sentiment_score'] = None\n",
        "\n",
        "total_rows = len(news)\n",
        "\n",
        "# Function to format the time remaining\n",
        "def format_time(seconds):\n",
        "    mins, secs = divmod(seconds, 60)\n",
        "    hours, mins = divmod(mins, 60)\n",
        "    return f\"{int(hours):02d}:{int(mins):02d}:{int(secs):02d}\"\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Iterate over the DataFrame rows to make API requests\n",
        "for index, row in news.iterrows():\n",
        "    time.sleep(0.4)  # Add a small delay to avoid overwhelming the API\n",
        "    print(f\"Processing row {index + 1} of {total_rows}\")\n",
        "    symbol = row['tickers']\n",
        "    timestamp = pd.Timestamp(row['date'])\n",
        "    date = timestamp.date()\n",
        "\n",
        "    # Uncomment the intraday fetching section if needed\n",
        "    \"\"\"\n",
        "    # Five minutes prior to the original timestamp\n",
        "    five_minutes_prior_timestamp_str = (timestamp - timedelta(minutes=5)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Original timestamp formatted\n",
        "    original_timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Fetch the intraday prices for the five minutes prior and concurrent timestamps\n",
        "    params_intraday = {\n",
        "        'symbol': symbol,\n",
        "        'interval': '1min',\n",
        "        'apikey': API_KEY,\n",
        "        'date': date,\n",
        "        'outputsize': 61  # Assuming 5 minutes is sufficient to cover both timestamps\n",
        "    }\n",
        "\n",
        "    response_intraday = requests.get(endpoint, params=params_intraday)\n",
        "    intraday_data = response_intraday.json()\n",
        "\n",
        "    # Extracting intraday price information\n",
        "    if 'values' in intraday_data:\n",
        "        for entry in intraday_data['values']:\n",
        "            entry_datetime_str = entry['datetime']\n",
        "            if entry_datetime_str == original_timestamp_str:\n",
        "                news.at[index, 'concurrent_price'] = entry['open']\n",
        "            elif entry_datetime_str == five_minutes_prior_timestamp_str:\n",
        "                news.at[index, 'five_minutes_prior_price'] = entry['open']\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch the day's high, low, open, and close prices\n",
        "    params_daily = {\n",
        "        'symbol': symbol,\n",
        "        'interval': '1day',\n",
        "        'apikey': API_KEY,\n",
        "        'date': date\n",
        "    }\n",
        "    response_daily = requests.get(endpoint, params=params_daily)\n",
        "    daily_data = response_daily.json()\n",
        "\n",
        "    # Update DataFrame with daily data\n",
        "    if 'values' in daily_data and daily_data['values']:\n",
        "        day_values = daily_data['values'][0]\n",
        "        news.at[index, 'open'] = day_values['open']\n",
        "        news.at[index, 'close'] = day_values['close']\n",
        "        news.at[index, 'high'] = day_values['high']\n",
        "        news.at[index, 'low'] = day_values['low']\n",
        "        news.at[index, 'sentiment_score'] = 100 * round((float(day_values['close']) - float(day_values['open'])) / float(day_values['open']), 3)\n",
        "\n",
        "    # Calculate progress\n",
        "    elapsed_time = time.time() - start_time\n",
        "    rows_processed = index + 1\n",
        "    percent_complete = (rows_processed / total_rows) * 100\n",
        "    time_per_row = elapsed_time / rows_processed\n",
        "    time_remaining = time_per_row * (total_rows - rows_processed)\n",
        "\n",
        "    print(f\"Progress: {percent_complete:.2f}% complete. Estimated time remaining: {format_time(time_remaining)}\")\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "news.to_excel(\"updated_pre_processed_news.xlsx\", index=False)\n",
        "print(\"Data processing complete. Updated file saved as 'updated_pre_processed_news.xlsx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8oOsdz6l2Ijh",
        "outputId": "34c698a7-7cd5-4d4e-f805-4ca4be4474bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing row 1 of 14507\n",
            "Progress: 0.01% complete. Estimated time remaining: 02:39:46\n",
            "Processing row 2 of 14507\n",
            "Progress: 0.01% complete. Estimated time remaining: 03:27:30\n",
            "Processing row 3 of 14507\n",
            "Progress: 0.02% complete. Estimated time remaining: 03:08:28\n",
            "Processing row 4 of 14507\n",
            "Progress: 0.03% complete. Estimated time remaining: 03:28:19\n",
            "Processing row 5 of 14507\n",
            "Progress: 0.03% complete. Estimated time remaining: 03:37:42\n",
            "Processing row 6 of 14507\n",
            "Progress: 0.04% complete. Estimated time remaining: 03:26:35\n",
            "Processing row 7 of 14507\n",
            "Progress: 0.05% complete. Estimated time remaining: 03:18:44\n",
            "Processing row 8 of 14507\n",
            "Progress: 0.06% complete. Estimated time remaining: 03:12:27\n",
            "Processing row 9 of 14507\n",
            "Progress: 0.06% complete. Estimated time remaining: 03:07:29\n",
            "Processing row 10 of 14507\n",
            "Progress: 0.07% complete. Estimated time remaining: 03:03:42\n",
            "Processing row 11 of 14507\n",
            "Progress: 0.08% complete. Estimated time remaining: 03:00:48\n",
            "Processing row 12 of 14507\n",
            "Progress: 0.08% complete. Estimated time remaining: 02:58:49\n",
            "Processing row 13 of 14507\n",
            "Progress: 0.09% complete. Estimated time remaining: 03:04:05\n",
            "Processing row 14 of 14507\n",
            "Progress: 0.10% complete. Estimated time remaining: 03:01:47\n",
            "Processing row 15 of 14507\n",
            "Progress: 0.10% complete. Estimated time remaining: 02:59:54\n",
            "Processing row 16 of 14507\n",
            "Progress: 0.11% complete. Estimated time remaining: 02:58:36\n",
            "Processing row 17 of 14507\n",
            "Progress: 0.12% complete. Estimated time remaining: 02:57:03\n",
            "Processing row 18 of 14507\n",
            "Progress: 0.12% complete. Estimated time remaining: 03:01:57\n",
            "Processing row 19 of 14507\n",
            "Progress: 0.13% complete. Estimated time remaining: 03:00:17\n",
            "Processing row 20 of 14507\n",
            "Progress: 0.14% complete. Estimated time remaining: 02:58:45\n",
            "Processing row 21 of 14507\n",
            "Progress: 0.14% complete. Estimated time remaining: 02:57:13\n",
            "Processing row 22 of 14507\n",
            "Progress: 0.15% complete. Estimated time remaining: 02:55:58\n",
            "Processing row 23 of 14507\n",
            "Progress: 0.16% complete. Estimated time remaining: 02:54:59\n",
            "Processing row 24 of 14507\n",
            "Progress: 0.17% complete. Estimated time remaining: 02:53:53\n",
            "Processing row 25 of 14507\n",
            "Progress: 0.17% complete. Estimated time remaining: 02:53:05\n",
            "Processing row 26 of 14507\n",
            "Progress: 0.18% complete. Estimated time remaining: 02:52:15\n",
            "Processing row 27 of 14507\n",
            "Progress: 0.19% complete. Estimated time remaining: 02:51:33\n",
            "Processing row 28 of 14507\n",
            "Progress: 0.19% complete. Estimated time remaining: 02:50:42\n",
            "Processing row 29 of 14507\n",
            "Progress: 0.20% complete. Estimated time remaining: 02:53:38\n",
            "Processing row 30 of 14507\n",
            "Progress: 0.21% complete. Estimated time remaining: 02:53:00\n",
            "Processing row 31 of 14507\n",
            "Progress: 0.21% complete. Estimated time remaining: 02:52:11\n",
            "Processing row 32 of 14507\n",
            "Progress: 0.22% complete. Estimated time remaining: 02:51:26\n",
            "Processing row 33 of 14507\n",
            "Progress: 0.23% complete. Estimated time remaining: 02:50:44\n",
            "Processing row 34 of 14507\n",
            "Progress: 0.23% complete. Estimated time remaining: 02:50:07\n",
            "Processing row 35 of 14507\n",
            "Progress: 0.24% complete. Estimated time remaining: 02:49:34\n",
            "Processing row 36 of 14507\n",
            "Progress: 0.25% complete. Estimated time remaining: 02:48:59\n",
            "Processing row 37 of 14507\n",
            "Progress: 0.26% complete. Estimated time remaining: 02:48:30\n",
            "Processing row 38 of 14507\n",
            "Progress: 0.26% complete. Estimated time remaining: 02:48:02\n",
            "Processing row 39 of 14507\n",
            "Progress: 0.27% complete. Estimated time remaining: 02:47:35\n",
            "Processing row 40 of 14507\n",
            "Progress: 0.28% complete. Estimated time remaining: 02:49:42\n",
            "Processing row 41 of 14507\n",
            "Progress: 0.28% complete. Estimated time remaining: 02:49:11\n",
            "Processing row 42 of 14507\n",
            "Progress: 0.29% complete. Estimated time remaining: 02:51:24\n",
            "Processing row 43 of 14507\n",
            "Progress: 0.30% complete. Estimated time remaining: 02:51:01\n",
            "Processing row 44 of 14507\n",
            "Progress: 0.30% complete. Estimated time remaining: 02:50:32\n",
            "Processing row 45 of 14507\n",
            "Progress: 0.31% complete. Estimated time remaining: 02:50:07\n",
            "Processing row 46 of 14507\n",
            "Progress: 0.32% complete. Estimated time remaining: 02:49:39\n",
            "Processing row 47 of 14507\n",
            "Progress: 0.32% complete. Estimated time remaining: 02:49:11\n",
            "Processing row 48 of 14507\n",
            "Progress: 0.33% complete. Estimated time remaining: 02:51:10\n",
            "Processing row 49 of 14507\n",
            "Progress: 0.34% complete. Estimated time remaining: 02:52:46\n",
            "Processing row 50 of 14507\n",
            "Progress: 0.34% complete. Estimated time remaining: 02:52:15\n",
            "Processing row 51 of 14507\n",
            "Progress: 0.35% complete. Estimated time remaining: 02:51:50\n",
            "Processing row 52 of 14507\n",
            "Progress: 0.36% complete. Estimated time remaining: 02:51:25\n",
            "Processing row 53 of 14507\n",
            "Progress: 0.37% complete. Estimated time remaining: 02:51:03\n",
            "Processing row 54 of 14507\n",
            "Progress: 0.37% complete. Estimated time remaining: 02:51:57\n",
            "Processing row 55 of 14507\n",
            "Progress: 0.38% complete. Estimated time remaining: 02:51:34\n",
            "Processing row 56 of 14507\n",
            "Progress: 0.39% complete. Estimated time remaining: 02:52:58\n",
            "Processing row 57 of 14507\n",
            "Progress: 0.39% complete. Estimated time remaining: 02:54:31\n",
            "Processing row 58 of 14507\n",
            "Progress: 0.40% complete. Estimated time remaining: 02:54:07\n",
            "Processing row 59 of 14507\n",
            "Progress: 0.41% complete. Estimated time remaining: 02:53:41\n",
            "Processing row 60 of 14507\n",
            "Progress: 0.41% complete. Estimated time remaining: 02:53:17\n",
            "Processing row 61 of 14507\n",
            "Progress: 0.42% complete. Estimated time remaining: 02:52:51\n",
            "Processing row 62 of 14507\n",
            "Progress: 0.43% complete. Estimated time remaining: 02:54:17\n",
            "Processing row 63 of 14507\n",
            "Progress: 0.43% complete. Estimated time remaining: 02:55:30\n",
            "Processing row 64 of 14507\n",
            "Progress: 0.44% complete. Estimated time remaining: 02:55:10\n",
            "Processing row 65 of 14507\n",
            "Progress: 0.45% complete. Estimated time remaining: 02:54:45\n",
            "Processing row 66 of 14507\n",
            "Progress: 0.45% complete. Estimated time remaining: 02:54:21\n",
            "Processing row 67 of 14507\n",
            "Progress: 0.46% complete. Estimated time remaining: 02:55:25\n",
            "Processing row 68 of 14507\n",
            "Progress: 0.47% complete. Estimated time remaining: 02:55:02\n",
            "Processing row 69 of 14507\n",
            "Progress: 0.48% complete. Estimated time remaining: 02:54:38\n",
            "Processing row 70 of 14507\n",
            "Progress: 0.48% complete. Estimated time remaining: 02:54:18\n",
            "Processing row 71 of 14507\n",
            "Progress: 0.49% complete. Estimated time remaining: 02:53:58\n",
            "Processing row 72 of 14507\n",
            "Progress: 0.50% complete. Estimated time remaining: 02:53:40\n",
            "Processing row 73 of 14507\n",
            "Progress: 0.50% complete. Estimated time remaining: 02:55:51\n",
            "Processing row 74 of 14507\n",
            "Progress: 0.51% complete. Estimated time remaining: 02:57:48\n",
            "Processing row 75 of 14507\n",
            "Progress: 0.52% complete. Estimated time remaining: 02:57:26\n",
            "Processing row 76 of 14507\n",
            "Progress: 0.52% complete. Estimated time remaining: 02:57:03\n",
            "Processing row 77 of 14507\n",
            "Progress: 0.53% complete. Estimated time remaining: 02:57:28\n",
            "Processing row 78 of 14507\n",
            "Progress: 0.54% complete. Estimated time remaining: 02:58:22\n",
            "Processing row 79 of 14507\n",
            "Progress: 0.54% complete. Estimated time remaining: 02:57:58\n",
            "Processing row 80 of 14507\n",
            "Progress: 0.55% complete. Estimated time remaining: 02:58:59\n",
            "Processing row 81 of 14507\n",
            "Progress: 0.56% complete. Estimated time remaining: 02:58:37\n",
            "Processing row 82 of 14507\n",
            "Progress: 0.57% complete. Estimated time remaining: 02:58:53\n",
            "Processing row 83 of 14507\n",
            "Progress: 0.57% complete. Estimated time remaining: 02:59:47\n",
            "Processing row 84 of 14507\n",
            "Progress: 0.58% complete. Estimated time remaining: 02:59:25\n",
            "Processing row 85 of 14507\n",
            "Progress: 0.59% complete. Estimated time remaining: 02:59:08\n",
            "Processing row 86 of 14507\n",
            "Progress: 0.59% complete. Estimated time remaining: 03:00:02\n",
            "Processing row 87 of 14507\n",
            "Progress: 0.60% complete. Estimated time remaining: 02:59:41\n",
            "Processing row 88 of 14507\n",
            "Progress: 0.61% complete. Estimated time remaining: 02:59:24\n",
            "Processing row 89 of 14507\n",
            "Progress: 0.61% complete. Estimated time remaining: 03:00:12\n",
            "Processing row 90 of 14507\n",
            "Progress: 0.62% complete. Estimated time remaining: 03:01:17\n",
            "Processing row 91 of 14507\n",
            "Progress: 0.63% complete. Estimated time remaining: 03:00:54\n",
            "Processing row 92 of 14507\n",
            "Progress: 0.63% complete. Estimated time remaining: 03:01:30\n",
            "Processing row 93 of 14507\n",
            "Progress: 0.64% complete. Estimated time remaining: 03:01:41\n",
            "Processing row 94 of 14507\n",
            "Progress: 0.65% complete. Estimated time remaining: 03:01:21\n",
            "Processing row 95 of 14507\n",
            "Progress: 0.65% complete. Estimated time remaining: 03:00:58\n",
            "Processing row 96 of 14507\n",
            "Progress: 0.66% complete. Estimated time remaining: 03:00:41\n",
            "Processing row 97 of 14507\n",
            "Progress: 0.67% complete. Estimated time remaining: 03:01:07\n",
            "Processing row 98 of 14507\n",
            "Progress: 0.68% complete. Estimated time remaining: 03:01:56\n",
            "Processing row 99 of 14507\n",
            "Progress: 0.68% complete. Estimated time remaining: 03:01:36\n",
            "Processing row 100 of 14507\n",
            "Progress: 0.69% complete. Estimated time remaining: 03:02:22\n",
            "Processing row 101 of 14507\n",
            "Progress: 0.70% complete. Estimated time remaining: 03:02:01\n",
            "Processing row 102 of 14507\n",
            "Progress: 0.70% complete. Estimated time remaining: 03:02:39\n",
            "Processing row 103 of 14507\n",
            "Progress: 0.71% complete. Estimated time remaining: 03:03:17\n",
            "Processing row 104 of 14507\n",
            "Progress: 0.72% complete. Estimated time remaining: 03:03:24\n",
            "Processing row 105 of 14507\n",
            "Progress: 0.72% complete. Estimated time remaining: 03:03:05\n",
            "Processing row 106 of 14507\n",
            "Progress: 0.73% complete. Estimated time remaining: 03:02:46\n",
            "Processing row 107 of 14507\n",
            "Progress: 0.74% complete. Estimated time remaining: 03:03:31\n",
            "Processing row 108 of 14507\n",
            "Progress: 0.74% complete. Estimated time remaining: 03:03:11\n",
            "Processing row 109 of 14507\n",
            "Progress: 0.75% complete. Estimated time remaining: 03:03:50\n",
            "Processing row 110 of 14507\n",
            "Progress: 0.76% complete. Estimated time remaining: 03:04:33\n",
            "Processing row 111 of 14507\n",
            "Progress: 0.77% complete. Estimated time remaining: 03:05:08\n",
            "Processing row 112 of 14507\n",
            "Progress: 0.77% complete. Estimated time remaining: 03:04:56\n",
            "Processing row 113 of 14507\n",
            "Progress: 0.78% complete. Estimated time remaining: 03:04:35\n",
            "Processing row 114 of 14507\n",
            "Progress: 0.79% complete. Estimated time remaining: 03:04:15\n",
            "Processing row 115 of 14507\n",
            "Progress: 0.79% complete. Estimated time remaining: 03:04:49\n",
            "Processing row 116 of 14507\n",
            "Progress: 0.80% complete. Estimated time remaining: 03:04:31\n",
            "Processing row 117 of 14507\n",
            "Progress: 0.81% complete. Estimated time remaining: 03:04:20\n",
            "Processing row 118 of 14507\n",
            "Progress: 0.81% complete. Estimated time remaining: 03:04:55\n",
            "Processing row 119 of 14507\n",
            "Progress: 0.82% complete. Estimated time remaining: 03:04:36\n",
            "Processing row 120 of 14507\n",
            "Progress: 0.83% complete. Estimated time remaining: 03:04:18\n",
            "Processing row 121 of 14507\n",
            "Progress: 0.83% complete. Estimated time remaining: 03:04:59\n",
            "Processing row 122 of 14507\n",
            "Progress: 0.84% complete. Estimated time remaining: 03:04:40\n",
            "Processing row 123 of 14507\n",
            "Progress: 0.85% complete. Estimated time remaining: 03:04:24\n",
            "Processing row 124 of 14507\n",
            "Progress: 0.85% complete. Estimated time remaining: 03:05:09\n",
            "Processing row 125 of 14507\n",
            "Progress: 0.86% complete. Estimated time remaining: 03:04:51\n",
            "Processing row 126 of 14507\n",
            "Progress: 0.87% complete. Estimated time remaining: 03:05:24\n",
            "Processing row 127 of 14507\n",
            "Progress: 0.88% complete. Estimated time remaining: 03:05:54\n",
            "Processing row 128 of 14507\n",
            "Progress: 0.88% complete. Estimated time remaining: 03:06:12\n",
            "Processing row 129 of 14507\n",
            "Progress: 0.89% complete. Estimated time remaining: 03:06:46\n",
            "Processing row 130 of 14507\n",
            "Progress: 0.90% complete. Estimated time remaining: 03:06:28\n",
            "Processing row 131 of 14507\n",
            "Progress: 0.90% complete. Estimated time remaining: 03:06:10\n",
            "Processing row 132 of 14507\n",
            "Progress: 0.91% complete. Estimated time remaining: 03:05:53\n",
            "Processing row 133 of 14507\n",
            "Progress: 0.92% complete. Estimated time remaining: 03:05:36\n",
            "Processing row 134 of 14507\n",
            "Progress: 0.92% complete. Estimated time remaining: 03:06:10\n",
            "Processing row 135 of 14507\n",
            "Progress: 0.93% complete. Estimated time remaining: 03:05:52\n",
            "Processing row 136 of 14507\n",
            "Progress: 0.94% complete. Estimated time remaining: 03:05:35\n",
            "Processing row 137 of 14507\n",
            "Progress: 0.94% complete. Estimated time remaining: 03:05:18\n",
            "Processing row 138 of 14507\n",
            "Progress: 0.95% complete. Estimated time remaining: 03:05:03\n",
            "Processing row 139 of 14507\n",
            "Progress: 0.96% complete. Estimated time remaining: 03:05:31\n",
            "Processing row 140 of 14507\n",
            "Progress: 0.97% complete. Estimated time remaining: 03:05:15\n",
            "Processing row 141 of 14507\n",
            "Progress: 0.97% complete. Estimated time remaining: 03:04:59\n",
            "Processing row 142 of 14507\n",
            "Progress: 0.98% complete. Estimated time remaining: 03:05:27\n",
            "Processing row 143 of 14507\n",
            "Progress: 0.99% complete. Estimated time remaining: 03:05:11\n",
            "Processing row 144 of 14507\n",
            "Progress: 0.99% complete. Estimated time remaining: 03:05:37\n",
            "Processing row 145 of 14507\n",
            "Progress: 1.00% complete. Estimated time remaining: 03:05:22\n",
            "Processing row 146 of 14507\n",
            "Progress: 1.01% complete. Estimated time remaining: 03:05:05\n",
            "Processing row 147 of 14507\n",
            "Progress: 1.01% complete. Estimated time remaining: 03:04:51\n",
            "Processing row 148 of 14507\n",
            "Progress: 1.02% complete. Estimated time remaining: 03:04:37\n",
            "Processing row 149 of 14507\n",
            "Progress: 1.03% complete. Estimated time remaining: 03:05:02\n",
            "Processing row 150 of 14507\n",
            "Progress: 1.03% complete. Estimated time remaining: 03:04:47\n",
            "Processing row 151 of 14507\n",
            "Progress: 1.04% complete. Estimated time remaining: 03:04:32\n",
            "Processing row 152 of 14507\n",
            "Progress: 1.05% complete. Estimated time remaining: 03:04:20\n",
            "Processing row 153 of 14507\n",
            "Progress: 1.05% complete. Estimated time remaining: 03:04:06\n",
            "Processing row 154 of 14507\n",
            "Progress: 1.06% complete. Estimated time remaining: 03:03:50\n",
            "Processing row 155 of 14507\n",
            "Progress: 1.07% complete. Estimated time remaining: 03:04:17\n",
            "Processing row 156 of 14507\n",
            "Progress: 1.08% complete. Estimated time remaining: 03:04:47\n",
            "Processing row 157 of 14507\n",
            "Progress: 1.08% complete. Estimated time remaining: 03:04:36\n",
            "Processing row 158 of 14507\n",
            "Progress: 1.09% complete. Estimated time remaining: 03:04:21\n",
            "Processing row 159 of 14507\n",
            "Progress: 1.10% complete. Estimated time remaining: 03:04:08\n",
            "Processing row 160 of 14507\n",
            "Progress: 1.10% complete. Estimated time remaining: 03:04:36\n",
            "Processing row 161 of 14507\n",
            "Progress: 1.11% complete. Estimated time remaining: 03:04:21\n",
            "Processing row 162 of 14507\n",
            "Progress: 1.12% complete. Estimated time remaining: 03:04:07\n",
            "Processing row 163 of 14507\n",
            "Progress: 1.12% complete. Estimated time remaining: 03:03:53\n",
            "Processing row 164 of 14507\n",
            "Progress: 1.13% complete. Estimated time remaining: 03:03:42\n",
            "Processing row 165 of 14507\n",
            "Progress: 1.14% complete. Estimated time remaining: 03:03:28\n",
            "Processing row 166 of 14507\n",
            "Progress: 1.14% complete. Estimated time remaining: 03:03:17\n",
            "Processing row 167 of 14507\n",
            "Progress: 1.15% complete. Estimated time remaining: 03:03:39\n",
            "Processing row 168 of 14507\n",
            "Progress: 1.16% complete. Estimated time remaining: 03:03:28\n",
            "Processing row 169 of 14507\n",
            "Progress: 1.16% complete. Estimated time remaining: 03:03:18\n",
            "Processing row 170 of 14507\n",
            "Progress: 1.17% complete. Estimated time remaining: 03:03:04\n",
            "Processing row 171 of 14507\n",
            "Progress: 1.18% complete. Estimated time remaining: 03:03:27\n",
            "Processing row 172 of 14507\n",
            "Progress: 1.19% complete. Estimated time remaining: 03:03:51\n",
            "Processing row 173 of 14507\n",
            "Progress: 1.19% complete. Estimated time remaining: 03:04:12\n",
            "Processing row 174 of 14507\n",
            "Progress: 1.20% complete. Estimated time remaining: 03:03:59\n",
            "Processing row 175 of 14507\n",
            "Progress: 1.21% complete. Estimated time remaining: 03:03:45\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-3f263eb6c0c6>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Iterate over the DataFrame rows to make API requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add a small delay to avoid overwhelming the API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing row {index + 1} of {total_rows}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msymbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tickers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}